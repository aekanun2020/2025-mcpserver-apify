#!/usr/bin/env python3

"""
Direct test script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö Facebook scraper functions ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á posts ‡πÅ‡∏•‡∏∞ comments scraping
"""

import asyncio
import json
import sys
import os

# Add the current directory to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from server import scrape_facebook_posts, scrape_facebook_comments, normalize_start_urls

async def test_normalize_function():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö normalize_start_urls function"""
    print("üß™ Testing normalize_start_urls function:")
    print("=" * 50)
    
    test_cases = [
        # Single string
        "https://www.facebook.com/NationTV/",
        
        # List of strings
        ["https://www.facebook.com/NationTV/", "https://www.facebook.com/ThaiPBS/"],
        
        # List of dicts
        [{"url": "https://www.facebook.com/NationTV/"}],
        
        # Mixed list
        ["https://www.facebook.com/NationTV/", {"url": "https://www.facebook.com/ThaiPBS/"}]
    ]
    
    for i, test_input in enumerate(test_cases, 1):
        print(f"\nüî¨ Test Case {i}:")
        print(f"Input: {test_input}")
        try:
            result = normalize_start_urls(test_input)
            print(f"Output: {result}")
            print("‚úÖ Success")
        except Exception as e:
            print(f"‚ùå Error: {e}")

async def test_posts_scraping():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ scrape Facebook posts ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á"""
    print("\n" + "=" * 50)
    print("üß™ Testing Facebook posts scraping function:")
    print("=" * 50)
    
    test_cases = [
        {
            "name": "Single page URL",
            "args": {
                "start_urls": "https://www.facebook.com/NationTV/",
                "results_limit": 2,
                "caption_text": False
            }
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nüî¨ Test {i}: {test_case['name']}")
        print(f"Args: {test_case['args']}")
        print("‚è≥ Scraping posts...")
        
        try:
            result = await scrape_facebook_posts(**test_case['args'])
            
            # Parse and display summary
            result_data = json.loads(result)
            if result_data.get("success"):
                print("‚úÖ Posts scraping successful!")
                print(f"   Total results: {result_data.get('totalResults', 0)}")
                print(f"   Input URLs: {result_data.get('inputUrls', [])}")
                print(f"   Run status: {result_data.get('runInfo', {}).get('status', 'unknown')}")
            else:
                print(f"‚ùå Posts scraping failed: {result_data.get('error')}")
                
        except Exception as e:
            print(f"‚ùå Exception occurred: {e}")

async def test_comments_scraping():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ scrape Facebook comments ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á"""
    print("\n" + "=" * 50)
    print("üß™ Testing Facebook comments scraping function:")
    print("=" * 50)
    
    test_cases = [
        {
            "name": "Single post URL",
            "args": {
                "start_urls": "https://www.facebook.com/humansofnewyork/posts/pfbid0BbKbkisExKGSKuhee9a7i86RwRuMKFC8NSkKStB7CsM3uXJuAAfZLrkcJMXxhH4Yl",
                "results_limit": 3,
                "include_nested_comments": False,
                "view_option": "RANKED_UNFILTERED"
            }
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nüî¨ Test {i}: {test_case['name']}")
        print(f"Args: {test_case['args']}")
        print("‚è≥ Scraping comments...")
        
        try:
            result = await scrape_facebook_comments(**test_case['args'])
            
            # Parse and display summary
            result_data = json.loads(result)
            if result_data.get("success"):
                print("‚úÖ Comments scraping successful!")
                print(f"   Total results: {result_data.get('totalResults', 0)}")
                print(f"   Input URLs: {result_data.get('inputUrls', [])}")
                print(f"   Run status: {result_data.get('runInfo', {}).get('status', 'unknown')}")
            else:
                print(f"‚ùå Comments scraping failed: {result_data.get('error')}")
                
        except Exception as e:
            print(f"‚ùå Exception occurred: {e}")

async def main():
    """Main test function"""
    print("""
üß™ Facebook Scraper Unified Direct Function Test
===============================================
‡∏ó‡∏î‡∏™‡∏≠‡∏ö unified functions ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô HTTP/SSE transport
""")
    
    # Test normalize function
    await test_normalize_function()
    
    # Test posts scraping
    await test_posts_scraping()
    
    # Wait between tests
    print("\n‚è≥ Waiting 5 seconds before comments test...")
    await asyncio.sleep(5)
    
    # Test comments scraping
    await test_comments_scraping()
    
    print("\n" + "=" * 50)
    print("üéâ Direct unified function tests completed!")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Test interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
